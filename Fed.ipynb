{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nuzhattttt/braintumor/blob/main/Fed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXvLJ53w6IqC",
        "outputId": "c4be0734-f97e-4edb-ac10-229c39e0b709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "PpSYiS908uQ8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_directory = '/content/gdrive/MyDrive/Amar dataset/dataset/Training'\n",
        "test_directory = '/content/gdrive/MyDrive/Amar dataset/dataset/Testing'"
      ],
      "metadata": {
        "id": "RjfPdAX6xn0F"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = (128, 128)\n",
        "train_images = []\n",
        "train_labels = []\n",
        "test_images = []\n",
        "test_labels = []"
      ],
      "metadata": {
        "id": "lPR2KR2rxno6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_images(directory, images_list, labels_list):\n",
        "    for class_name in os.listdir(directory):\n",
        "        class_directory = os.path.join(directory, class_name)\n",
        "\n",
        "        for filename in os.listdir(class_directory):\n",
        "            if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "                image_path = os.path.join(class_directory, filename)\n",
        "                image = Image.open(image_path).convert(\"RGB\")\n",
        "                image = image.resize(image_size, Image.LANCZOS)\n",
        "                images_list.append(image)\n",
        "                labels_list.append(class_name)\n"
      ],
      "metadata": {
        "id": "UhsdO0Gc0_sH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import backend as K\n",
        "def create_clients(data_list, label_list, num_clients=10, initial='clients'):\n",
        "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
        "    data = list(zip(data_list, label_list))\n",
        "    random.shuffle(data)\n",
        "    size = len(data)//num_clients\n",
        "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
        "    assert(len(shards) == len(client_names))\n",
        "    return {client_names[i] : shards[i] for i in range(len(client_names))}\n",
        "def batch_data(data_shard, bs=32):\n",
        "    data, label = zip(*data_shard)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
        "    return dataset.shuffle(len(label)).batch(bs)\n",
        "class SimpleMLP:\n",
        "    @staticmethod\n",
        "    def build(shape, classes):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(500,input_shape=(shape,)))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Dense(300))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Dense(200))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Dense(classes))\n",
        "        model.add(Activation(\"sigmoid\"))\n",
        "        return model\n",
        "class VGG16:\n",
        "    @staticmethod\n",
        "    def build(shape, classes):\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(input_shape=shape,filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "        model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "        model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "        model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "        model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "        model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "        model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "        model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "        model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "        model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "        model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "        model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "        model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "        model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "        model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "        model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "        model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "        model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(units=4096,activation=\"relu\"))\n",
        "        model.add(Dense(units=4096,activation=\"relu\"))\n",
        "        model.add(Dense(units=classes, activation=\"softmax\"))\n",
        "        return model\n",
        "\n",
        "def weight_scalling_factor(clients_trn_data, client_name):\n",
        "    client_names = list(clients_trn_data.keys())\n",
        "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
        "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
        "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
        "    return local_count/global_count\n",
        "\n",
        "def scale_model_weights(weight, scalar):\n",
        "    weight_final = []\n",
        "    steps = len(weight)\n",
        "    for i in range(steps):\n",
        "        weight_final.append(scalar * weight[i])\n",
        "    return weight_final\n",
        "\n",
        "def sum_scaled_weights(scaled_weight_list):\n",
        "    avg_grad = list()\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\n",
        "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
        "        avg_grad.append(layer_mean)\n",
        "    return avg_grad\n",
        "\n",
        "def test_model(X_test, Y_test,  model, comm_round):\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss = cce(Y_test, logits)\n",
        "    acc = accuracy_score( tf.argmax(Y_test, axis=1),tf.argmax(logits, axis=1))\n",
        "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
        "    return acc, loss"
      ],
      "metadata": {
        "id": "z5av6xhcjUed"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_and_preprocess_images(train_directory, train_images, train_labels)\n"
      ],
      "metadata": {
        "id": "DbPXRf6g1bma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_and_preprocess_images(test_directory, test_images, test_labels)\n"
      ],
      "metadata": {
        "id": "vu1enbJ02OEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_labels)"
      ],
      "metadata": {
        "id": "bjrdfh2V79nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "train_encoded_labels = label_encoder.fit_transform(train_labels)\n",
        "test_encoded_labels = label_encoder.transform(test_labels)"
      ],
      "metadata": {
        "id": "3nuhfHQx2nk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")"
      ],
      "metadata": {
        "id": "IYEbn41Jqw7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array([np.array(img) for img in train_images])\n",
        "y_train = np.array(train_encoded_labels)"
      ],
      "metadata": {
        "id": "ktzAVpXmuMqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_train))"
      ],
      "metadata": {
        "id": "33sT3wBGwsnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_images = []\n",
        "\n",
        "for image in X_train:\n",
        "    augmented_image = datagen.random_transform(image)\n",
        "    augmented_images.append(augmented_image)\n",
        "X_train_augmented = np.array(augmented_images)"
      ],
      "metadata": {
        "id": "M1akID5ArPn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_augmented.shape)"
      ],
      "metadata": {
        "id": "Z5wxUuB9wgQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.array([np.array(img) for img in test_images])\n",
        "y_test = np.array(test_encoded_labels)\n"
      ],
      "metadata": {
        "id": "9auYL7pX3-rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "CePHIht6BbHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "labels = list(set(label_list.tolist()))\n",
        "#lb = LabelBinarizer()\n",
        "#label_list = lb.fit_transform(label_list)\n",
        "n_values = np.max(label_list) + 1\n",
        "label_list = np.eye(n_values)[label_list]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_list,\n",
        "                                                    label_list,\n",
        "                                                    test_size=0.1,\n",
        "                                                    random_state=42)\n",
        "\n",
        "clients = create_clients(X_train, y_train, num_clients=10, initial='client')\n",
        "\n"
      ],
      "metadata": {
        "id": "ZY_hyds3ilFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test)"
      ],
      "metadata": {
        "id": "KiLcb6oXB7UE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clients_batched = dict()\n",
        "for (client_name, data) in clients.items():\n",
        "    clients_batched[client_name] = batch_data(data)\n",
        "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))\n",
        "print(test_batched)"
      ],
      "metadata": {
        "id": "oJ_1-l42yTjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(clients_batched)"
      ],
      "metadata": {
        "id": "KSIrYQtUCL3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(labels)"
      ],
      "metadata": {
        "id": "1zP05-tw6z0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers.legacy import SGD\n",
        "comms_round = 10\n",
        "#create optimizer\n",
        "lr = 0.01\n",
        "loss='categorical_crossentropy'\n",
        "metrics = ['accuracy']\n",
        "# optimizer = Adam(learning_rate=lr,\n",
        "#                 weight_decay=lr / comms_round,\n",
        "#                 ema_momentum=0.9\n",
        "#                )\n",
        "optimizer = SGD(lr=lr,\n",
        "                decay=lr / comms_round,\n",
        "                momentum=0.9\n",
        "               )\n",
        "\n",
        "\n",
        "#initialize global model\n",
        "#print(data_list.shape,labels)\n",
        "# smlp_global = SimpleMLP()\n",
        "# global_model = smlp_global.build(data_list[0].shape,len(labels))\n",
        "\n",
        "smpl_vgg = VGG16()\n",
        "global_model = smpl_vgg.build(data_list[0].shape,len(labels))"
      ],
      "metadata": {
        "id": "qMWgzLDg0qn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(global_model.get_weights())"
      ],
      "metadata": {
        "id": "hj6TrpAq9_2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for comm_round in range(comms_round):\n",
        "    global_weights = global_model.get_weights()\n",
        "    scaled_local_weight_list = list()\n",
        "    client_names= list(clients_batched.keys())\n",
        "    random.shuffle(client_names)\n",
        "    for client in tqdm(client_names , desc = 'Progress Bar'):\n",
        "        #time.sleep(0.5)\n",
        "        smpl_local = VGG16()\n",
        "        local_model = smpl_vgg.build(data_list[0].shape,len(labels))\n",
        "        smlp_local = SimpleMLP()\n",
        "        local_model.compile(loss=loss,\n",
        "                      optimizer=optimizer,\n",
        "                      metrics=metrics)\n",
        "        #print(local_model.summary())\n",
        "        #print(clients_batched)\n",
        "        local_model.set_weights(global_weights)\n",
        "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
        "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
        "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
        "        scaled_local_weight_list.append(scaled_weights)\n",
        "        K.clear_session()\n",
        "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
        "    global_model.set_weights(average_weights)\n",
        "    for(X_test, Y_test) in test_batched:\n",
        "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)\n",
        "SGD_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(y_train)).batch(320)\n",
        "smlp_SGD = VGG16()\n",
        "SGD_model = smlp_SGD.build(data_list[0].shape,len(labels))\n",
        "SGD_model.compile(loss=loss,\n",
        "              optimizer=optimizer,\n",
        "              metrics=metrics)\n",
        "_ = SGD_model.fit(SGD_dataset, epochs=100, verbose=0)\n",
        "for(X_test, Y_test) in test_batched:\n",
        "        SGD_acc, SGD_loss = test_model(X_test, Y_test, SGD_model, 1)"
      ],
      "metadata": {
        "id": "J1jjRU9Q2Q_A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}